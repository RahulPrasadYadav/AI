{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5412a3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"\"\n",
    "      The rain had not stopped for three days. It drummed relentlessly against the metal roof of the small, cramped outpost in Sector 4. Elara sat by the window, watching the grey water run down the glass in crooked rivulets. She was an Archivist, one of the few remaining individuals trained to read physical paper in a world that had moved entirely to digital consciousness. Her job was simple but tedious: she had to sort through the remnants of the Old World, cataloging every scrap of paper, every photograph, and every handwritten note that the scavengers brought in from the ruins of the cities beyond the wall.\n",
    "\n",
    "It was a lonely job. Most people in the city did not understand why she cared about the dusty, crumbling relics of the past. To them, information was something that was beamed directly into their neural implants. It was instant, clean, and efficient. But to Elara, there was a magic in the tangible. She loved the smell of old books, the scent of vanilla and almonds that rose from the pages when she opened them. She loved the feel of the texture under her fingertips, the way the ink faded over time, turning from a sharp black to a soft charcoal grey.\n",
    "\n",
    "On this particular Tuesday, the shipment of artifacts was larger than usual. A scavenger team had broken into a sealed basement in what used to be a library. They had brought back three large crates, sealed with rusted padlocks. Elara picked up her laser cutter and carefully sliced through the metal of the first lock. It fell to the floor with a heavy thud. She lifted the lid and peered inside. The box was filled with journals. Dozens of them. They were bound in leather, some cracked and peeling, others surprisingly well-preserved.\n",
    "\n",
    "She reached in and pulled out the one on top. It was a small, black notebook. She opened the cover. On the first page, written in a frantic, jagged scrawl, were the words: \"If you are reading this, they have already found me.\" A chill ran down Elara's spine. This was not just a record of grocery lists or weather patterns; this was a story. She sat down in her chair, adjusting the old lamp on her desk, and began to read.\n",
    "\n",
    "The journal belonged to a man named Elias. He had lived during the Transition, the period fifty years ago when the government mandated the switch to the Neural Network. Elias was a resistor. He did not want a chip in his brain. He did not want his thoughts to be uploaded to the cloud. He wanted to remain private. As Elara read, she found herself using the same stopwords that she usually ignored. She read \"the\" fear in his words. She felt \"a\" sense of panic. The little words—and, but, or, for—connected his fragmented thoughts into a coherent stream of terror.\n",
    "\n",
    "\"I cannot stay here,\" Elias had written on page ten. \"The drones are scanning the sector. I heard them buzzing outside my window last night. It is a sound like angry hornets. I have packed my bag. I am taking only the essentials. Water, dried fruit, and this notebook. I need to leave a record. I need someone to know that we existed. That we were not just data points to be deleted.\"\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2530d3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b33b3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c1b9eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae55b9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stmmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4f12dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49378ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "santences=nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c14b21b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(santences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07e5462c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yrahu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yrahu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Apply stopword filtering and then stemming\n",
    "# Ensure required NLTK resources are available\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for i in range(len(santences)):\n",
    "    words = nltk.word_tokenize(santences[i])\n",
    "    # keep alphabetic tokens, lower-case for matching, then stem\n",
    "    filtered = [stmmer.stem(word.lower()) for word in words if word.isalpha() and word.lower() not in stop_words]\n",
    "    santences[i] = ' '.join(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d56c1179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rain stop three day',\n",
       " 'drum relentlessli metal roof small cramp outpost sector',\n",
       " 'elara sat window watch grey water run glass crook rivulet',\n",
       " 'archivist one remain individu train read physic paper world move entir digit conscious',\n",
       " 'job simpl tediou sort remnant old world catalog everi scrap paper everi photograph everi handwritten note scaveng brought ruin citi beyond wall',\n",
       " 'lone job',\n",
       " 'peopl citi understand care dusti crumbl relic past',\n",
       " 'inform someth beam directli neural implant',\n",
       " 'instant clean effici',\n",
       " 'elara magic tangibl',\n",
       " 'love smell old book scent vanilla almond rose page open',\n",
       " 'love feel textur fingertip way ink fade time turn sharp black soft charcoal grey',\n",
       " 'particular tuesday shipment artifact larger usual',\n",
       " 'scaveng team broken seal basement use librari',\n",
       " 'brought back three larg crate seal rust padlock',\n",
       " 'elara pick laser cutter care slice metal first lock',\n",
       " 'fell floor heavi thud',\n",
       " 'lift lid peer insid',\n",
       " 'box fill journal',\n",
       " 'dozen',\n",
       " 'bound leather crack peel other surprisingli',\n",
       " 'reach pull one top',\n",
       " 'small black notebook',\n",
       " 'open cover',\n",
       " 'first page written frantic jag scrawl word read alreadi found',\n",
       " 'chill ran elara spine',\n",
       " 'record groceri list weather pattern stori',\n",
       " 'sat chair adjust old lamp desk began read',\n",
       " 'journal belong man name elia',\n",
       " 'live transit period fifti year ago govern mandat switch neural network',\n",
       " 'elia resistor',\n",
       " 'want chip brain',\n",
       " 'want thought upload cloud',\n",
       " 'want remain privat',\n",
       " 'elara read found use stopword usual ignor',\n",
       " 'read fear word',\n",
       " 'felt sens panic',\n",
       " 'littl fragment thought coher stream terror',\n",
       " 'stay elia written page ten',\n",
       " 'drone scan sector',\n",
       " 'heard buzz outsid window last night',\n",
       " 'sound like angri hornet',\n",
       " 'pack bag',\n",
       " 'take essenti',\n",
       " 'water dri fruit notebook',\n",
       " 'need leav record',\n",
       " 'need someon know exist',\n",
       " 'data point delet']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "santences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbabd20a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
